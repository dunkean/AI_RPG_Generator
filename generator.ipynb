{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import query\n",
    "import prompt as P\n",
    "from parsers import json_parser, table_parser\n",
    "from logger import Logger\n",
    "from llm import ChatGPT\n",
    "import defines as D\n",
    "import data_formatter as DF\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "PROJECT_ID = \"Enclave\"\n",
    "LORE = \"\"\"A medieval fantasy world. Only one God exists (the god is non-gendered and called \"the Old One\") but people believes also in local beliefs. No undead. A single big nation (empire) on a unique continent the size of europe. \"\"\"\n",
    "GROUP_DESCRIPTION = \"Near an ancient forest, far from the city, a small quiet and prosperous village surrounded by a lush valley. There are ruins in the forest, and the village is near a river. Mountains can be seen in the distance. The village was founded centuries ago by groups of scholars and adventurers who came to study a long forgotten event. The legacy of the ancient settlers still remains faintly in the village, and the ruins of the ancient forest remain a mystery. Villains, monsters and creatures are pretty rare in the area, so the adventurers are not many but the village is still protected by a small militia and trade is going well.\"\n",
    "GROUP_DESCRIPTION = \"Near a big calm lake, far from the city, a small quiet and prosperous village surrounded by a mountain range. The village was founded centuries ago by groups of strangers. The legacy of the ancient settlers still remains faintly in the village. Monsters and creatures are lurking in the mountains, so the villagers know how to defend themselves.\"\n",
    "GROUP_DESCRIPTION = \"Mistwood is an hamlet nestled within a dense forest shrouded in perpetual mist, this small village exudes an ethereal charm. Cottages with thatched roofs line cobblestone streets, adorned with colorful flowers. The villagers, known for their mystical abilities, harness the power of the forest. Luminescent creatures flit between trees, while wisps of enchantment linger in the air. A mystical well at the village center grants healing waters, drawing pilgrims seeking rejuvenation. The village thrives in harmony with nature, its inhabitants practicing ancient arts. Tales of hidden groves, magical artifacts, and the occasional fae encounter make this village a treasure trove of enchantment.\"\n",
    "GROUP_DESCRIPTION = \"Deep within the heart of the mountain, a small group of resilient dwarf miners toils tirelessly in the depths of the earth. Their home, Shadowvein hall is a testament to their unwavering dedication to their craft. Carved from the living rock, the hall is adorned with shimmering crystals and veins of precious ore, casting a warm, golden glow. The rhythmic sound of pickaxes echoes through the tunnels as the miners unearth rich veins of silver, gold, and gemstones. In the flickering light of the forges, skilled artisans shape the raw materials into intricate works of art, crafting exquisite jewelry and ornate weapons. Despite the dangers that lurk within the dark tunnels, these stout-hearted dwarves stand united, their camaraderie and resilience forming the bedrock of their enduring legacy.\"\n",
    "GROUP_DESCRIPTION = \"Concealed within the depths of a mountain forest, a small elven settlement lies hidden from prying eyes. Accessed only by winding paths and secret passages, this ethereal haven emanates an air of enchantment and mystique. Elven dwellings, intricately woven from living vines and adorned with vibrant blossoms, blend seamlessly with the surrounding flora. Soft sunlight filters through the dense canopy, casting dappled patterns upon moss-covered paths. The residents, graceful and attuned to nature, practice ancient elven arts, weaving intricate spells and communing with forest spirits. Echoes of elven songs and laughter drift through the glades, while hidden groves reveal shimmering pools where the moon's reflection reveals ancient prophecies. Silvanvale Refuge stands as a sanctuary of elven wisdom, a place where time seems to slow and the harmony of the natural world thrives undisturbed.\"\n",
    "GROUP_DESCRIPTION = \"Ravenmoor City: Rising like a foreboding monolith from the surrounding landscape, Ravenmoor City looms with an air of mystery and trepidation. Its towering spires pierce the darkened sky, casting elongated shadows over the sprawling suburbs. The city sprawls along the banks of a vast, murky lake, its waters reflecting the city's somber atmosphere. The cobblestone streets are littered with refuse and grime, intertwining with the stench of rot and decay that hangs heavy in the air.\"\n",
    "GROUP_DESCRIPTION = \"Nestled in the countryside near a vast lake, Lakeview Haven stands as a bustling town along a major trading route, attracting adventurers seeking glory amidst the perilous plains and forest infested with green-skinned creatures. While the town thrives with prosperity and vibrant markets, its suburban outskirts tell a different taleâ€”dirty and dangerous alleyways where poverty-stricken residents struggle amidst dilapidated buildings, casting a shadow over the town's otherwise flourishing reputation as a beacon of hope in the wilderness.\"\n",
    "GROUP_DESCRIPTION = \"Deep within the heart of the formidable mountains lies The Forgotten Enclave, a resilient community eking out an existence within the vast ruins of a once-majestic city. Cut off from the outside world, these settlers and beggars have embraced their precarious existence, fashioning crude shelters amidst crumbling architecture and overgrown foliage. Silent whispers of a lost civilization echo through the labyrinthine streets, while weathered statues and dilapidated structures stand as testament to a grandeur long past. Life in The Forgotten Enclave is a constant struggle for survival, as residents scavenge for meager resources, relying on their resourcefulness and communal bonds to endure within this hauntingly beautiful, forgotten realm.\"\n",
    "WORLD_TYPE = \"Medieval fantasy\"\n",
    "INSTRUCTION = f\"You are an assistant who generates on-demand data for a tabletop {WORLD_TYPE} role-playing game\"\n",
    "SCALE = \"local\"\n",
    "TYPE = \"small community\"\n",
    "POPULATION = 80\n",
    "# SEED = 42\n",
    "# ratio = {\"human\": 0.75, \"elf\": 0.05, \"half-elf\": 0.05, \"dwarf\": 0.1, \"halfling\": 0.025, \"gnome\": 0.025}\n",
    "RACE_RATIO = {\"human\": 0.6, \"half-elf\": 0.15, \"elf\":0.05, \"dwarf\": 0.15, \"halfling\": 0.025, \"gnome\": 0.025}\n",
    "\n",
    "Logger = Logger()\n",
    "\n",
    "FORCE_HUMAN_READABLE_CACHE = True\n",
    "\n",
    "\n",
    "with open(\"openai.key\", \"r\") as f:\n",
    "    api_key = f.readline().strip()\n",
    "    api_id = f.readline().strip()\n",
    "    \n",
    "LLM = ChatGPT(Logger, INSTRUCTION, model_name=\"gpt-3.5-turbo\", api_key=api_key, api_id=api_id)\n",
    "Q = query.LLM_Query(LLM, Logger)\n",
    "\n",
    "\n",
    "# Bootstrap\n",
    "content = DF.bootstrap_content(\n",
    "    PROJECT_ID, LORE, TYPE, SCALE, POPULATION, GROUP_DESCRIPTION, WORLD_TYPE)\n",
    "prompt = P.prompt_bootstrap(content)\n",
    "Logger.log(prompt, id=PROJECT_ID, type=\"bootstrap_prompt\")\n",
    "\n",
    "new_content = Q.query(prompt, json_parser, id=PROJECT_ID, type=\"bootstrap\", force_human_readable_cache=FORCE_HUMAN_READABLE_CACHE)\n",
    "Logger.log_json(new_content, id=PROJECT_ID, type=\"bootstrap\")\n",
    "\n",
    "content = DF.format_bootstrap_query(content, new_content)\n",
    "\n",
    "# Generate details\n",
    "# TODO multithread\n",
    "categories_to_details = [\"customs\", \"resources\", \"history\", \"external_influences\", \"timeline\", \"sites\", \"anecdotes\"]\n",
    "for category in categories_to_details:\n",
    "    # print(category)\n",
    "    prompt = P.prompt_category(content, category)\n",
    "    new_content = Q.query(prompt, json_parser, id=PROJECT_ID, type=f\"{category}\", force_human_readable_cache=FORCE_HUMAN_READABLE_CACHE)\n",
    "    Logger.log(prompt, id=PROJECT_ID, type=f\"{category}_prompt\")\n",
    "    Logger.log_json(new_content, id=PROJECT_ID, type=f\"{category}\")\n",
    "\n",
    "    content = DF.format_category_query(content, new_content, category)\n",
    "\n",
    "\n",
    "prompt = P.prompt_wk2(content)\n",
    "Logger.log(prompt, id=PROJECT_ID, type=f\"wk2_prompt\")\n",
    "new_content = Q.query(prompt, table_parser, id=PROJECT_ID, type=\"wk2\", force_human_readable_cache=FORCE_HUMAN_READABLE_CACHE)\n",
    "Logger.log_json(new_content, id=PROJECT_ID, type=\"wk2\")\n",
    "# content[\"workplaces\"] = new_content\n",
    "print(len(new_content))\n",
    "sum_pop_needed = sum([int(v[\"population\"]) for v in new_content])\n",
    "print(sum_pop_needed)\n",
    "content[\"workplaces\"] = new_content\n",
    "\n",
    "\n",
    "with open(f\"output/{PROJECT_ID}.json\", \"w\") as f:\n",
    "    json.dump(content, f, indent=4, sort_keys=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from numpy import random as npr\n",
    "import json\n",
    "import generator as G\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "ind_filename = os.path.join(\"output\", f\"{PROJECT_ID}_individuals_pool.json\")\n",
    "group_filename = os.path.join(\"output\", f\"{PROJECT_ID}_groups_pool.json\")\n",
    "# print(ind_filename, group_filename)\n",
    "\n",
    "individuals_pool = None\n",
    "if os.path.exists(ind_filename):\n",
    "    with open(ind_filename, \"r\") as infile:\n",
    "        individuals_pool = json.load(infile)\n",
    "        print(\"loading inds from file\")\n",
    "\n",
    "groups_pool = None\n",
    "if os.path.exists(group_filename):\n",
    "    with open(group_filename, \"r\") as infile:\n",
    "        groups_pool = json.load(infile)\n",
    "        print(\"loading groups from file\")\n",
    "\n",
    "# generate reference population (family and solos)\n",
    "while (individuals_pool is None) or (groups_pool is None) or len(individuals_pool) < sum_pop_needed:\n",
    "    print(\"Generating pools...\")\n",
    "    natives, outsiders, groups = G.get_native_populations(content)\n",
    "    families = G.family_distribution(natives*1.4, RACE_RATIO)\n",
    "    f_individuals = {f\"{m['name']} {m['surname']}\": m for g in families for m in g[\"members\"]}\n",
    "    solos = G.outsiders_distribution(outsiders*1.4, f_individuals, RACE_RATIO)\n",
    "    s_individuals = {f\"{m['name']} {m['surname']}\": m for g in solos for m in g[\"members\"]}\n",
    "    individuals = {**f_individuals, **s_individuals}\n",
    "    individuals_pool = individuals.copy()\n",
    "    groups_pool = [*families.copy(), *solos.copy()]\n",
    "\n",
    "with open(ind_filename, \"w\") as f:\n",
    "    json.dump(individuals_pool, f, indent=4, sort_keys=True)\n",
    "\n",
    "with open(group_filename, \"w\") as f:\n",
    "    json.dump(groups_pool, f, indent=4, sort_keys=True)\n",
    "\n",
    "print(\"Individuals pool\", len(individuals_pool))\n",
    "print(\"Groups pool\", len(groups_pool))\n",
    "\n",
    "\n",
    "# Distribute groups to activities\n",
    "workplaces = content[\"workplaces\"]\n",
    "group_list = groups_pool.copy()\n",
    "\n",
    "grp_per_wk_filename = os.path.join(\"output\", f\"{PROJECT_ID}_grp_per_wk.json\")\n",
    "grp_per_wk = None\n",
    "if os.path.exists(grp_per_wk_filename):\n",
    "    with open(grp_per_wk_filename, \"r\") as infile:\n",
    "        grp_per_wk = json.load(infile)\n",
    "        print(\"loading grp_per_wk from file\")\n",
    "else:\n",
    "    grp_per_wk = {}\n",
    "    for wk in workplaces:\n",
    "        chosen_groups, chosen_details = G.preferential_group_selection(wk[\"composition\"].lower(), wk[\"type\"].lower(), wk[\"ages\"].lower(), int(wk[\"population\"]), group_list, individuals_pool)\n",
    "        grp_per_wk[wk[\"name\"]] = [c[0] for c in chosen_details]\n",
    "\n",
    "    with open(grp_per_wk_filename, \"w\") as f:\n",
    "        json.dump(grp_per_wk, f, indent=4, sort_keys=True)\n",
    "\n",
    "for grp in groups_pool:\n",
    "    grp[\"workplace\"] = []\n",
    "\n",
    "if grp_per_wk is not None:\n",
    "    for k, v in grp_per_wk.items():\n",
    "        for grp in groups_pool:\n",
    "            if grp[\"name\"] in v:\n",
    "                grp[\"workplace\"] = k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "threads = []\n",
    "responses = [[] for _ in range(len(groups_pool))]\n",
    "group_indexes = {}\n",
    "\n",
    "\n",
    "def people_group_query(prompt, nam, index, res):\n",
    "\n",
    "    Logger.log(prompt, id=PROJECT_ID, type=f\"{nam}_ingroup_prompt\")\n",
    "    new_content = Q.query(prompt, table_parser, id=PROJECT_ID, type=f\"{nam}_ingroup\")\n",
    "    Logger.log_json(new_content, id=PROJECT_ID, type=f\"{nam}_ingroup\")\n",
    "    res[index] = [*res[index], *new_content]\n",
    "\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "for i, group in enumerate(groups_pool):\n",
    "    group_indexes[group[\"name\"]] = i\n",
    "    for subset in range(0, len(group[\"members\"]), BATCH_SIZE):\n",
    "        group_to_query = group.copy()\n",
    "        group_to_query[\"members\"] = group_to_query[\"members\"][subset:subset+BATCH_SIZE]\n",
    "        grp_workplaces = [w for w in content[\"workplaces\"] if w[\"name\"] == group[\"workplace\"]]\n",
    "        workplace = grp_workplaces[0] if len(grp_workplaces) > 0 else None\n",
    "        prompt = P.prompt_people_in_groups(content, group_to_query, workplace)\n",
    "        nam = group[\"name\"]\n",
    "        cache_content = Q.get_cache_type(\"project\", prompt, table_parser, id=PROJECT_ID, type=f\"{nam}_ingroup\")\n",
    "\n",
    "        if cache_content is not None:\n",
    "            responses[i] = [*responses[i], *cache_content]\n",
    "            threads.append((i, None))\n",
    "        else:\n",
    "            threads.append(\n",
    "                (i, Thread(target=people_group_query, args=[prompt, nam, i, responses])))\n",
    "    # break\n",
    "\n",
    "for i, thread in threads:\n",
    "    if thread is None:\n",
    "        continue\n",
    "    print(f\"Starting group query job {i} - {groups_pool[i]['name']}...\")\n",
    "    thread.start()\n",
    "    time.sleep(40)\n",
    "\n",
    "print(f\"{len(threads)} group query job started...\")\n",
    "\n",
    "for i, thread in threads:\n",
    "    if thread is None:\n",
    "        continue\n",
    "    thread.join()\n",
    "\n",
    "\n",
    "print(\"Group query jobs done\")\n",
    "content[\"groups\"] = {grp[\"name\"]: grp for grp in groups_pool}\n",
    "for k, v in group_indexes.items():\n",
    "    content[\"groups\"][k][\"generated_members\"] = responses[v]\n",
    "\n",
    "# FORMAT / CLEAN RESPONSES\n",
    "for k, v in content[\"groups\"].items():\n",
    "    for m in v[\"generated_members\"]:\n",
    "        found = 0\n",
    "        for n in v[\"members\"]:\n",
    "            if n[\"name\"] in m[\"name\"]:\n",
    "                found += 1\n",
    "                n.update(m)\n",
    "                break\n",
    "\n",
    "    for m in v[\"members\"]:\n",
    "        if m['surname'] in m['name']:\n",
    "            m['name'] = m['name'].replace(m['surname'], \"\").strip()\n",
    "        m['fullname'] = f\"{m['name']} {m['surname']}\"\n",
    "\n",
    "    if \"generated_members\" in v:\n",
    "        del v[\"generated_members\"]\n",
    "    if \"detailed_members\" in v:\n",
    "        del v[\"detailed_members\"]\n",
    "    \n",
    "\n",
    "\n",
    "with open(f\"output/{PROJECT_ID}.json\", \"w\") as f:\n",
    "    json.dump(content, f, indent=4, sort_keys=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "employed = {} # {workplace: [members]}\n",
    "unemployed = {}\n",
    "employed_filename = os.path.join(\"output\", f\"{PROJECT_ID}_employed.json\")\n",
    "unemployed_filename = os.path.join(\"output\", f\"{PROJECT_ID}_unemployed.json\")\n",
    "\n",
    "if os.path.exists(employed_filename) and os.path.exists(unemployed_filename):\n",
    "    with open(employed_filename, \"r\") as infile:\n",
    "        employed = json.load(infile)\n",
    "    with open(unemployed_filename, \"r\") as infile:\n",
    "        unemployed = json.load(infile)\n",
    "else:\n",
    "    \n",
    "    for k, v in content[\"groups\"].items():\n",
    "        v[\"members\"] = [m for m in v[\"members\"] if \"structure_preference\" in m]\n",
    "        \n",
    "    individual_pool = {f\"{member['fullname']}\": member for _, group in content[\"groups\"].items() for member in group[\"members\"]}\n",
    "    wk_group_members_pool = {\n",
    "        wk['name']: {\n",
    "            grp_name: {f\"{member['fullname']}\":member for member in grp[\"members\"]}\n",
    "            for grp_name, grp in content[\"groups\"].items() if wk[\"name\"] in grp[\"workplace\"]\n",
    "            } for wk in content[\"workplaces\"]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for i, workplace in enumerate(content[\"workplaces\"]):\n",
    "        wk_employed = G.preferential_individual_selection(workplace, individual_pool, wk_group_members_pool)\n",
    "        employed[workplace[\"name\"]] = wk_employed\n",
    "        \n",
    "    unemployed = individual_pool.copy()\n",
    "    \n",
    "    with open(employed_filename, \"w\") as f:\n",
    "        json.dump(employed, f, indent=4, sort_keys=True)\n",
    "    with open(unemployed_filename, \"w\") as f:\n",
    "        json.dump(unemployed, f, indent=4, sort_keys=True)\n",
    "\n",
    "print(\"Employed and unemployed loaded\", sum([len(v) for v in list(employed.values())]), len(unemployed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "threads = []\n",
    "responses = [[] for _ in content[\"workplaces\"]]\n",
    "wk_indexes = {}\n",
    "\n",
    "\n",
    "def workplace_employees_query(wk, employees, i, responses):\n",
    "    prompt = P.prompt_workplace_employees(content, wk, employees)\n",
    "    nam = wk[\"name\"]\n",
    "    Logger.log(prompt, id=PROJECT_ID, type=f\"{nam}_wk_emp_prompt\")\n",
    "    new_content = Q.query(prompt, table_parser, id=PROJECT_ID, type=f\"{nam}_wk_emp\", force_human_readable_cache=True)\n",
    "    Logger.log_json(new_content, id=PROJECT_ID, type=f\"{nam}_wk_emp\")\n",
    "    responses[i] = new_content\n",
    "\n",
    "\n",
    "for i, wk in enumerate(content[\"workplaces\"]):\n",
    "    wk_indexes[wk[\"name\"]] = i\n",
    "    employees = employed[wk[\"name\"]]\n",
    "    prompt = P.prompt_workplace_employees(content, wk, employees)\n",
    "    nam = wk[\"name\"]\n",
    "    cache_content = Q.get_cache_type(\"project\", prompt, table_parser, id=PROJECT_ID, type=f\"{nam}_wk_emp_prompt\")\n",
    "\n",
    "    if cache_content is not None:\n",
    "        responses[i] = [*responses[i], *cache_content]\n",
    "        threads.append((i, None))\n",
    "    else:\n",
    "        threads.append((i, Thread(target=workplace_employees_query, args=[wk, employees, i, responses])))\n",
    "    # break\n",
    "\n",
    "for i, thread in threads:\n",
    "    if thread is None:\n",
    "        continue\n",
    "    print(f\"\\nStarting group query job {i}\", end=\"...\")\n",
    "    thread.start()\n",
    "    time.sleep(0.2)\n",
    "    if len(responses[i]) > 0:\n",
    "        print(f\"Cache found for {i}\", end=\"\")\n",
    "        continue\n",
    "    time.sleep(25)\n",
    "\n",
    "print(f\"{len(threads)} group query job started...\")\n",
    "\n",
    "for i, thread in threads:\n",
    "    if thread is None:\n",
    "        continue\n",
    "    thread.join()\n",
    "\n",
    "workplaces_employees = {k: responses[v] for k, v in wk_indexes.items()}\n",
    "for wk, emp in workplaces_employees.items():\n",
    "    for m in emp:\n",
    "        m['fullname'] = m['name']\n",
    "        del m['name']\n",
    "\n",
    "content[\"employees\"] = workplaces_employees\n",
    "with open(f\"output/{PROJECT_ID}.json\", \"w\") as f:\n",
    "    json.dump(content, f, indent=4, sort_keys=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "threads = []\n",
    "responses = [[] for _ in content[\"workplaces\"]]\n",
    "wk_indexes = {}\n",
    "\n",
    "\n",
    "def workplace_details_query(wk, employees, i, responses):\n",
    "    prompt = P.prompt_workplace_details(content, wk, employees)\n",
    "    # print(prompt)\n",
    "    nam = wk[\"name\"]\n",
    "    Logger.log(prompt, id=PROJECT_ID, type=f\"{nam}_wk_dtls_prompt\")\n",
    "    new_content = Q.query(prompt, json_parser, id=PROJECT_ID, type=f\"{nam}_wk_dtls\")\n",
    "    Logger.log_json(new_content, id=PROJECT_ID, type=f\"{nam}_wk_dtls\")\n",
    "    responses[i] = new_content\n",
    "\n",
    "\n",
    "for i, wk in enumerate(content[\"workplaces\"]):\n",
    "    wk_indexes[wk[\"name\"]] = i\n",
    "    employees = employed[wk[\"name\"]]\n",
    "    wk_employees = {e[\"fullname\"]:e for e in workplaces_employees[wk[\"name\"]]}\n",
    "    to_delete = []\n",
    "    for v in employees:\n",
    "        if v[\"fullname\"] not in wk_employees:\n",
    "            to_delete.append(v)\n",
    "            # print(v[\"fullname\"])\n",
    "        else:\n",
    "            v.update(wk_employees[v[\"fullname\"]])\n",
    "    for v in to_delete:\n",
    "        employees.remove(v)\n",
    "    \n",
    "    prompt = P.prompt_workplace_details(content, wk, employees)\n",
    "    nam = wk[\"name\"]\n",
    "    cache_content = Q.get_cache_type(\"project\", prompt, json_parser, id=PROJECT_ID, type=f\"{nam}_wk_dtls\")\n",
    "    if cache_content is not None:\n",
    "        responses[i] = cache_content\n",
    "        threads.append((i, None))\n",
    "    else:\n",
    "        threads.append((i, Thread(target=workplace_details_query, args=[wk, employees, i, responses])))\n",
    "\n",
    "for i, thread in threads:\n",
    "    if thread is None:\n",
    "        continue\n",
    "    print(f\"Starting group query job {i}\")\n",
    "    thread.start()\n",
    "    time.sleep(25)\n",
    "\n",
    "print(f\"{len(threads)} group query job started...\")\n",
    "\n",
    "for i, thread in threads:\n",
    "    if thread is None:\n",
    "        continue\n",
    "    thread.join()\n",
    "\n",
    "# print(responses)\n",
    "workplaces_details = {k: responses[v] for k, v in wk_indexes.items()}\n",
    "content[\"wk_details\"] = workplaces_details\n",
    "with open(f\"output/{PROJECT_ID}.json\", \"w\") as f:\n",
    "    json.dump(content, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(employed_filename, \"w\") as f:\n",
    "    json.dump(employed, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format sites workplaces\n",
    "\n",
    "for k, v in content[\"wk_details\"].items():\n",
    "    new_wk_sites = {}\n",
    "    #test if sites is a list\n",
    "    if isinstance(v[\"sites\"], list):\n",
    "        for obj in v[\"sites\"]:\n",
    "            # print(obj)\n",
    "            if \"keywords\" in obj:\n",
    "                key_names = [key for key in obj.keys() if key.find('name') != -1]\n",
    "                if len(key_names) > 0:\n",
    "                    # print(obj[key_names[0]], obj[\"keywords\"])\n",
    "                    new_wk_sites[obj[key_names[0]]] = obj[\"keywords\"]\n",
    "            elif len(obj.keys()) == 1:\n",
    "                key_name = list(obj.keys())[0]\n",
    "                # print(key_name, obj[key_name])\n",
    "                new_wk_sites[key_name] = obj[key_name]\n",
    "            else:\n",
    "                for key in obj.keys():\n",
    "                    print(key, obj[key])\n",
    "                    new_wk_sites[key] = obj[key]\n",
    "        v[\"sites\"] = new_wk_sites\n",
    "        \n",
    "\n",
    "\n",
    "with open(f\"output/{PROJECT_ID}.json\", \"w\") as f:\n",
    "    json.dump(content, f, indent=4, sort_keys=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "prompt = P.prompt_architecture_and_sites(content)\n",
    "Logger.log(prompt, id=PROJECT_ID, type=f\"architecture_prompt\")\n",
    "new_content = Q.query(prompt, json_parser, id=PROJECT_ID, type=\"architecture\", force_human_readable_cache=FORCE_HUMAN_READABLE_CACHE)\n",
    "Logger.log_json(new_content, id=PROJECT_ID, type=\"architecture\")\n",
    "print(new_content)\n",
    "content[\"architecture\"] = new_content\n",
    "with open(f\"output/{PROJECT_ID}.json\", \"w\") as f:\n",
    "    json.dump(content, f, indent=4, sort_keys=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "threads = []\n",
    "responses = [[] for _ in content[\"workplaces\"]]\n",
    "wk_indexes = {}\n",
    "\n",
    "\n",
    "def workplace_sites_query(wk, wk_details, i, responses):\n",
    "    prompt = P.prompt_sites(content, wk, wk_details)\n",
    "    # print(prompt)\n",
    "    nam = wk[\"name\"]\n",
    "    Logger.log(prompt, id=PROJECT_ID, type=f\"{nam}_wk_site_prompt\")\n",
    "    new_content = Q.query(prompt, json_parser, id=PROJECT_ID, type=f\"{nam}_wk_site\")\n",
    "    Logger.log_json(new_content, id=PROJECT_ID, type=f\"{nam}_wk_site\")\n",
    "    responses[i] = new_content\n",
    "\n",
    "\n",
    "for i, wk in enumerate(content[\"workplaces\"]):\n",
    "    wk_indexes[wk[\"name\"]] = i\n",
    "    wk_details = content[\"wk_details\"][wk[\"name\"]]\n",
    "    \n",
    "    prompt = P.prompt_sites(content, wk, wk_details)\n",
    "    # print(prompt)\n",
    "    nam = wk[\"name\"]\n",
    "    cache_content = Q.get_cache_type(\"project\", prompt, json_parser, id=PROJECT_ID, type=f\"{nam}_wk_site\")\n",
    "    if cache_content is not None:\n",
    "        responses[i] = cache_content\n",
    "        threads.append((i, None))\n",
    "    else:\n",
    "        threads.append((i, Thread(target=workplace_sites_query, args=[wk, wk_details, i, responses])))\n",
    "\n",
    "for i, thread in threads:\n",
    "    if thread is None:\n",
    "        continue\n",
    "    print(f\"Starting group query job {i}\")\n",
    "    thread.start()\n",
    "    time.sleep(25)\n",
    "\n",
    "print(f\"{len(threads)} group query job started...\")\n",
    "\n",
    "for i, thread in threads:\n",
    "    if thread is None:\n",
    "        continue\n",
    "    thread.join()\n",
    "\n",
    "# print(responses)\n",
    "workplaces_details = {k: responses[v] for k, v in wk_indexes.items()}\n",
    "content[\"wk_sites\"] = workplaces_details\n",
    "with open(f\"output/{PROJECT_ID}.json\", \"w\") as f:\n",
    "    json.dump(content, f, indent=4, sort_keys=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_str = json.dumps(content, indent=4, sort_keys=True)\n",
    "for k, v in content[\"wk_details\"].items():\n",
    "    content_str = content_str.replace(k, v[\"new_name\"])\n",
    "    # print(k, \"|\", v[\"new_name\"])\n",
    "\n",
    "with open(f\"output/{PROJECT_ID}_renamed.json\", \"w\") as f:\n",
    "    f.write(content_str)\n",
    "    \n",
    "content = json.loads(content_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_figures = []\n",
    "\n",
    "for fname, family in content[\"groups\"].items():\n",
    "    for member in family[\"members\"]:\n",
    "        if \"key_figure\" not in member:\n",
    "            pass\n",
    "            # print(\"****\", fname, member[\"fullname\"], family[\"name\"])\n",
    "        elif member[\"key_figure\"] == \"yes\":\n",
    "            for wname, employees in content[\"employees\"].items():\n",
    "                if member[\"fullname\"] in [e[\"fullname\"] for e in employees]:\n",
    "                    for workplace in content[\"workplaces\"]:\n",
    "                        if workplace[\"name\"] == wname:\n",
    "                            key_figures.append((member, employees, family, workplace))\n",
    "            \n",
    "\n",
    "for wname, employees in content[\"employees\"].items():\n",
    "    for mb in employees:\n",
    "        if \"key_figure\" not in mb or mb[\"key_figure\"] == \"no\":\n",
    "            continue\n",
    "        # print(\"-----\", mb)\n",
    "        member = None\n",
    "        for fname, family in content[\"groups\"].items():\n",
    "            for fmember in family[\"members\"]:\n",
    "                if fmember[\"fullname\"] == mb[\"fullname\"]:\n",
    "                    member = fmember\n",
    "                    break\n",
    "        if member is None:\n",
    "            continue\n",
    "\n",
    "        for workplace in content[\"workplaces\"]:\n",
    "            if workplace[\"name\"] == wname:\n",
    "                is_present = False\n",
    "                for it in key_figures:\n",
    "                    if it[0][\"fullname\"] == member[\"fullname\"]:\n",
    "                        is_present = True\n",
    "                if not is_present:\n",
    "                    key_figures.append((member, employees, family, workplace))\n",
    "\n",
    "threads = []\n",
    "responses = [[] for _ in key_figures]\n",
    "mb_indexes = {}\n",
    "\n",
    "print(len(key_figures), \"key figures\")\n",
    "\n",
    "def member_details_query(mb, colleagues, family, workplace, i, responses):\n",
    "    prompt = P.prompt_member_details(content, mb, colleagues, family, workplace)\n",
    "    nam = mb[\"fullname\"]\n",
    "    Logger.log(prompt, id=PROJECT_ID, type=f\"{nam}_member_details_prompt\")\n",
    "    new_content = Q.query(prompt, json_parser, id=PROJECT_ID, type=f\"{nam}_member_details\")\n",
    "    Logger.log_json(new_content, id=PROJECT_ID, type=f\"{nam}_member_details\")\n",
    "    responses[i] = new_content\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i, (mb, colleagues, family, workplace) in enumerate(key_figures):\n",
    "    mb_indexes[mb[\"fullname\"]] = i\n",
    "    prompt = P.prompt_member_details(content, mb, colleagues, family, workplace)\n",
    "    nam = mb[\"fullname\"]\n",
    "    cache_content = Q.get_cache_type(\"project\", prompt, json_parser, id=PROJECT_ID, type=f\"{nam}_member_details\")\n",
    "    if cache_content is not None:\n",
    "        responses[i] = cache_content\n",
    "        threads.append((i, None))\n",
    "    else:\n",
    "        threads.append((i, Thread(target=member_details_query, args=[mb, colleagues, family, workplace, i, responses])))\n",
    "\n",
    "\n",
    "for i, thread in threads:\n",
    "    if thread is None:\n",
    "        continue\n",
    "    print(f\"Starting member query job {i}\")\n",
    "    thread.start()\n",
    "    time.sleep(25)\n",
    "\n",
    "print(f\"{len(threads)} member query job started...\")\n",
    "\n",
    "for i, thread in threads:\n",
    "    if thread is None:\n",
    "        continue\n",
    "    thread.join()\n",
    "\n",
    "members_details = {k: responses[v] for k, v in mb_indexes.items()}\n",
    "# print(members_details)\n",
    "content[\"members_details\"] = members_details\n",
    "with open(f\"output/{PROJECT_ID}.json\", \"w\") as f:\n",
    "    json.dump(content, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy as dc\n",
    "\n",
    "bak_content = dc(content)\n",
    "rcontent = {}\n",
    "\n",
    "rcontent[\"project_id\"] = content[\"project_id\"]\n",
    "rcontent[\"name\"] = content[\"name\"]\n",
    "rcontent[\"scale\"] = content[\"scale\"]\n",
    "rcontent[\"size\"] = content[\"size\"]\n",
    "rcontent[\"type\"] = content[\"type\"]\n",
    "rcontent[\"structure\"] = content[\"structure\"]\n",
    "rcontent[\"lore\"] = content[\"lore\"]\n",
    "\n",
    "rcontent[\"details\"] = {\n",
    "    \"description\": content[\"details\"],\n",
    "    \"anecdotes\": content[\"anecdotes\"],\n",
    "    \"architecture\": content[\"architecture\"], ## merge with sites\n",
    "    \"culture\": content[\"culture\"],\n",
    "    \"customs\": content[\"customs\"],\n",
    "    \"goals\": content[\"goals\"],\n",
    "    \"history\": content[\"history\"],\n",
    "    \"resources\": content[\"resources\"],\n",
    "    \"timeline\": content[\"timeline\"],\n",
    "    \"external_influences\": content[\"external_influences\"]\n",
    "}\n",
    "\n",
    "rcontent[\"details\"][\"architecture\"][\"details\"] = content[\"sites\"][\"details\"]\n",
    "rcontent[\"details\"][\"architecture\"][\"keywords\"] = content[\"sites\"][\"keywords\"]\n",
    "rcontent[\"details\"][\"architecture\"][\"style\"] = rcontent[\"details\"][\"architecture\"][\"architecture\"]\n",
    "del rcontent[\"details\"][\"architecture\"][\"architecture\"]\n",
    "\n",
    "\n",
    "npcs = {}\n",
    "\n",
    "rcontent[\"groups\"] = {}\n",
    "for k, v in content[\"groups\"].items():\n",
    "    # print(v)\n",
    "    rcontent[\"groups\"][k] = v.copy()\n",
    "    rcontent[\"groups\"][k][\"members\"] = []\n",
    "    rcontent[\"groups\"][k][\"key_figures\"] = []\n",
    "    for member in v[\"members\"]:\n",
    "        npcs[member[\"fullname\"]] = member.copy()\n",
    "        rcontent[\"groups\"][k][\"members\"].append(member[\"fullname\"])\n",
    "        if member.get(\"key_figure\", \"no\") == \"yes\":\n",
    "            rcontent[\"groups\"][k][\"key_figures\"].append(member[\"fullname\"])\n",
    "\n",
    "wks = {}\n",
    "for v in content[\"workplaces\"]:\n",
    "    k = v[\"name\"]\n",
    "    wks[k] = v.copy()\n",
    "    wks[k][\"employees\"] = []\n",
    "    wks[k][\"key_figures\"] = []\n",
    "    for employee in content[\"employees\"][k]:\n",
    "        fullname = employee[\"fullname\"]\n",
    "        wks[k][\"employees\"].append(fullname)\n",
    "        if fullname not in npcs:\n",
    "            # check if a family is related to workspace and test name\n",
    "            for fname, family in content[\"groups\"].items():\n",
    "                if k in family[\"workplace\"]:\n",
    "                    fullname = fullname + \" \" + fname\n",
    "                    if fullname in npcs:\n",
    "                        npcs[fullname][\"job\"] = employee.copy()\n",
    "                    else:\n",
    "                        print(\"Unable to find thus removing:\", fullname, \"in npcs for workplace:\", k, \"| group:\", fname, \"| generated:\", fullname)\n",
    "        else:\n",
    "            npcs[fullname][\"job\"] = employee.copy()\n",
    "            npcs[fullname][\"job\"][\"workplace\"] = k\n",
    "            \n",
    "        if employee.get(\"key_figure\", \"no\") == \"yes\":\n",
    "            wks[k][\"key_figures\"].append(fullname)\n",
    "        \n",
    "print(\"NPCs:\", len(npcs))\n",
    "print(\"NPCs with job:\", len([k for k, v in npcs.items() if \"job\" in v]))\n",
    "print(\"NPCs without job:\", len([k for k, v in npcs.items() if \"job\" not in v]))\n",
    "\n",
    "\n",
    "for k, w in content[\"wk_details\"].items():\n",
    "    v = w.copy()\n",
    "    v[\"old_name\"] = v[\"name\"]\n",
    "    del v[\"new_name\"]\n",
    "    del v[\"name\"]\n",
    "    wks[k].update(v)\n",
    "    wks[k][\"sites\"] = content[\"wk_sites\"][k]\n",
    "\n",
    "rcontent[\"workplaces\"] = wks\n",
    "\n",
    "for wk in rcontent[\"workplaces\"].values():\n",
    "    if isinstance(wk[\"anecdotes\"], list):\n",
    "        wk[\"anecdotes\"] = \". \".join(wk[\"anecdotes\"])\n",
    "    if isinstance(wk[\"plot\"], list):\n",
    "        wk[\"plot\"] = \". \".join(wk[\"plot\"])\n",
    "    wk[\"keywords\"] = wk[\"keywords\"].replace('\\\"', '')\n",
    "\n",
    "for fullname, npc in npcs.items():\n",
    "    if \"description\" in npc:\n",
    "        npc[\"short_description\"] = npc[\"description\"]\n",
    "    npc[\"family\"] = {\n",
    "        \"key_figure\": npc.get(\"key_figure\", \"no\"),\n",
    "        \"rank\": npc.get(\"rank\", npc[\"situation\"]),\n",
    "        \"relationship\": npc.get(\"relationship\", \"\"),\n",
    "        \"situation\": npc[\"situation\"],\n",
    "    }\n",
    "    for k in [\"key_figure\", \"rank\", \"relationship\", \"situation\"]:\n",
    "        if k in npc:\n",
    "            del npc[k]\n",
    "    if \"job\" in npc:\n",
    "        if \"working clothes\" in npc[\"job\"]:\n",
    "            npc[\"job\"][\"working_clothes\"] = npc[\"job\"][\"working clothes\"]\n",
    "            del npc[\"job\"][\"working clothes\"]\n",
    "\n",
    "for k, v in content[\"members_details\"].copy().items():\n",
    "    v[\"description\"] = v[\"desc\"]\n",
    "    v[\"goals\"] = v[\"goals_keywords\"]\n",
    "    del v[\"goals_keywords\"]\n",
    "    del v[\"desc\"]\n",
    "    del v[\"fullname\"]\n",
    "    npcs[k].update(v)\n",
    "    \n",
    "rcontent[\"npcs\"] = npcs\n",
    "\n",
    "with open(f\"output/{PROJECT_ID}_r.json\", \"w\") as f:\n",
    "    json.dump(rcontent, f, indent=4, sort_keys=True)\n",
    "    \n",
    "content = bak_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f\"output/{PROJECT_ID}_r.json\", \"r\") as f:\n",
    "    rcontent = json.load(f)\n",
    "    \n",
    "scrap_json = dc(rcontent)\n",
    "\n",
    "\n",
    "def clean_json(json):\n",
    "    if isinstance(json, dict):\n",
    "        for k, v in json.items():\n",
    "            if isinstance(v, str):\n",
    "                if json[k] != \"etc...\":\n",
    "                    json[k] = \"\"\n",
    "            elif isinstance(v, list):\n",
    "                json[k] = v[:1]\n",
    "                clean_json(json[k])\n",
    "            elif isinstance(v, dict):\n",
    "                if not any([ki in v.keys() for ki in [\"details\", \"keywords\", \"fullname\", \"description\", \"desc\", \"name\"]]):                    \n",
    "                    json[k] = {f\"{k}_name_1\": vv for kk, vv in v.items() if kk == list(v.keys())[-1]}\n",
    "                    json[k][f\"{k}_name_2\"] = \"etc...\"\n",
    "                clean_json(json[k])\n",
    "            elif isinstance(v, int):\n",
    "                pass\n",
    "            else:\n",
    "                print(k, v, \"not a type\")\n",
    "\n",
    "    elif isinstance(json, list):\n",
    "        for k, v in enumerate(json):\n",
    "            if isinstance(v, str):\n",
    "                if json[k] != \"etc...\":\n",
    "                    json[k] = \"\"\n",
    "            elif isinstance(v, list):\n",
    "                json[k] = v[:1]\n",
    "                clean_json(json[k])\n",
    "            elif isinstance(v, dict):\n",
    "                if not any([ki in v.keys() for ki in [\"details\", \"keywords\", \"fullname\", \"description\", \"desc\", \"rank\"]]):\n",
    "                    json[k] = {f\"{k}_name_1\": vv for kk, vv in v.items() if kk == list(v.keys())[-1]}\n",
    "                    json[k][f\"{k}_name_2\"] = \"etc...\"\n",
    "                clean_json(json[k])\n",
    "            elif isinstance(v, int):\n",
    "                pass\n",
    "            else:\n",
    "                print(k, v, \"not a type\")\n",
    "\n",
    "clean_json(scrap_json)\n",
    "print(json.dumps(scrap_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webuiapi\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "import base64\n",
    "import requests\n",
    "api = webuiapi.WebUIApi(host='127.0.0.1', port=7860)\n",
    "\n",
    "\n",
    "OUTPUT = f\"output/{PROJECT_ID}\"\n",
    "folder_list = [\"\", \"/portraits\", \"/portraits/prompts\", \"/sites\", \"/sites/prompts\"]\n",
    "for folder in folder_list:\n",
    "    if not os.path.exists(OUTPUT + folder):\n",
    "        os.makedirs(OUTPUT + folder)\n",
    "    \n",
    "    \n",
    "def submit_post(url: str, data: dict):\n",
    "    \"\"\"\n",
    "    Submit a POST request to the given URL with the given data.\n",
    "    \"\"\"\n",
    "    return requests.post(url, data=json.dumps(data))\n",
    "\n",
    "\n",
    "def save_encoded_image(b64_image: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Save the given image to the given output path.\n",
    "    \"\"\"\n",
    "    with open(output_path, \"wb\") as image_file:\n",
    "        image_file.write(base64.b64decode(b64_image))\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as i:\n",
    "        b64 = base64.b64encode(i.read())\n",
    "    return b64.decode(\"utf-8\")\n",
    "\n",
    "\n",
    "api.util_set_model(\"dreamlikeDiffusion10_10.ckpt [0aecbcfa2c]\")\n",
    "# api.util_set_model('elldrethSLucidMix_v10.safetensors [67abd65708]')\n",
    "# api.util_set_model('realisticVisionV13_v13.safetensors [c35782bad8]')\n",
    "# api.util_set_model(\"RPG-v4.safetensors [e04b020012]\")\n",
    "# api.util_set_model('realisticVisionV13_v13.safetensors [c35782bad8]')\n",
    "\n",
    "# negative_prompt = \"(clones), (((twins))), (((double persons))), couple, (double people), blur, ((dual face)), ((double face)), (((poorly drawn eyes))), (mutated eyes), (poorly drawn hand), (mutated hand), text, watermark, bad art, (playing card), card, ((out of frame)) (low resolution). blurred. ((3D render)) warped, ((watermark)), extra fingers, mutated hands, ((poorly drawn hands)), ((extra limbs)), cloned face, gross proportions, (malformed limbs), ((missing arms)), ((missing legs)), (((extra arms))), (((extra legs))), mutated hands, (fused fingers)\"\n",
    "\n",
    "negative_prompt = \"((3d)), ((asian)), low resolution, blurred, badhandv4:1.3, ((watermark)), extra fingers, mutated hands, ((poorly drawn hands)), ((extra limbs)), (malformed limbs), ((missing arms)), ((missing legs)), (((extra arms))), (((extra legs))), mutated hands, ((fused fingers)), ((poorly drawn eyes)), (mutated eyes), (poorly drawn hand), (mutated hand), (clones), (((twins))), (((double person))), couple, (double people), blur, ((dual face)), ((double face)), ((two pairs of hears)), ((cloned hears))\"\n",
    "\n",
    "# , mutated eyes, ((poorly drawn eyes)), (low quality eyes):1.3\n",
    "# (mutated eyes:1.2), \n",
    "\n",
    "# negative_prompt = \"((out of frame)), ((3d)), ((asian)), low resolution, blurred, badhandv4:1.3, ((watermark)), extra fingers, mutated hands, ((poorly drawn hands)), ((extra limbs)), (malformed limbs), ((missing arms)), ((missing legs)), (((extra arms))), (((extra legs))), mutated hands, ((fused fingers)), ((poorly drawn eyes)), (mutated eyes), (poorly drawn hand), (mutated hand), (clones), (((twins))), (((double person))), couple, (double people), blur, ((dual face)), ((double face)), ((two pairs of hears)), ((cloned hears))\"\n",
    "\n",
    "def render_prompt(npc_name, npc, environment):\n",
    "    clothes = npc[\"job\"][\"working_clothes\"] if \"job\" in npc else npc[\"clothes\"]\n",
    "    job = \"\" if \"job\" not in npc else npc[\"job\"][\"job\"]\n",
    "    p = npc\n",
    "    \n",
    "    # prompt = f\"\"\"((heroic fantasy)), masterpiece, ((front full body view)) of a ({p[\"age_look\"]}:1.3) ({p[\"gender\"]}) ({p[\"race\"]}) ({job}) with a (less than {p[\"beauty\"]}:1.3 face) ({p[\"beauty\"]}:1.3 appearance), {clothes}, {p[\"weight\"]} weight, {p[\"height\"]} size, {p[\"eyes\"]} eyes, {p[\"hair\"]} hair, {p[\"skin\"]} skin, by greg rutkowski, ({p[\"age_look\"]}:1.2 character), photorealistic, realistic eyes, detailed eyes, extremely detailed eyes, cinematic lighting, ((masterpiece)), sharp focus, ({environment} background)\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"((heroic fantasy)), masterpiece, fine art painting, rpg book illustration, ((front full body view)) of a ({p[\"age_look\"]}:1.3) ({p[\"gender\"]}) ({p[\"race\"]}) ({job}) with a ({p[\"beauty\"]} face):1.3 and a ({p[\"beauty\"]}:1.3 appearance), poor and ragged {clothes}, {p[\"weight\"]} weight, {p[\"height\"]} size, (({p[\"beauty\"]}:1.3 figure), {p[\"eyes\"]} eyes, {p[\"hair\"]} hair, {p[\"skin\"]} skin, (({p[\"beauty\"]}:1.3 character) by greg rutkowski, photorealistic, realistic eyes, realistic skin, (detailed eyes), (extremely detailed eyes), cinematic lighting, ((masterpiece)), sharp focus, ({environment} background), <lora:epiNoiseoffset_v2:1>\"\"\"\n",
    "        \n",
    "    # prompt = f\"\"\"masterpiece, best quality, ultra-detailed, fine art painting, photorealistic, ultra realistic, illustration, heroic fantasy, dungeons and dragons, ({p[\"age_look\"]}):1.3 {p[\"gender\"]}:1.1 ({p[\"race\"]} race):1.3 ({job}) with a ({p[\"beauty\"]} face):1.3, {clothes}, {p[\"weight\"]} weight, {p[\"height\"]} size, {p[\"eyes\"]} eyes, {p[\"hair\"]} hair, {p[\"skin\"]} skin, ({p[\"age_look\"]} character):1.3, by greg rutkowski and Thomas Benjamin Kennington, sharp focus, cinematic lighting, photorealistic:1.2,  <lora:epiNoiseoffset_v2:1>, {environment} background, matte, bokeh, fantasy background, realistic eyes, detailed eyes, ultra detailed, extremely detailed, 8 k, hdr, hd\"\"\"\n",
    "    \n",
    "    \n",
    "    prompt_name = f\"{OUTPUT}/portraits/prompts/{npc_name.strip()}.txt\"\n",
    "\n",
    "    with open(prompt_name, \"w\") as outfile:\n",
    "        outfile.write(prompt)\n",
    "        outfile.write(\"\\n\\n\")\n",
    "        outfile.write(negative_prompt)\n",
    "            \n",
    "    return prompt\n",
    "\n",
    "def render(prompt, p):\n",
    "    # print(prompt)\n",
    "    seed = random.randint(0, 1000000)\n",
    "    \n",
    "    width = 576\n",
    "    height = 768\n",
    "    render_options = {}\n",
    "    if \"job\" in p:\n",
    "        render_options = {\n",
    "            \"enable_hr\": True,\n",
    "            \"denoising_strength\": 0.5,\n",
    "            \"hr_scale\": 2,\n",
    "            \"hr_upscaler\": \"Latent\",\n",
    "            \"hr_resize_x\": width*2,\n",
    "            \"hr_resize_y\": height*2,\n",
    "        }\n",
    "    \n",
    "    \n",
    "    img_names = [f\"{p['fullname']}\", f\"{p['fullname'].strip()}\", \" \".join(p[\"fullname\"].split(\" \")[::-1]), \" \".join(p[\"fullname\"].split(\" \")[::-1]).strip()]\n",
    "    for img_name in img_names:\n",
    "        img_path = f\"{OUTPUT}/portraits/{img_name}.jpg\"\n",
    "        if os.path.exists(img_path):\n",
    "            return\n",
    "    \n",
    "    print(\"Generating\", p[\"fullname\"])\n",
    "    data = {\"prompt\": prompt,\n",
    "            \"negative_prompt\": negative_prompt,\n",
    "            # \"init_images\": [image],\t# For img2img\n",
    "            \"sampler_name\": 'DPM++ 2M Karras',\n",
    "            \"seed\": -1,\n",
    "            \"cfg_scale\": 4.5,\n",
    "            \"n_iter\": 1,\n",
    "            \"steps\": 40,\n",
    "            \"batch_size\": 1,\n",
    "            \"width\": width,\n",
    "            \"height\": height,\n",
    "            \"restore_faces\": True,\n",
    "            **render_options\n",
    "            \n",
    "            # \"alwayson_scripts\": {\n",
    "            #     \"controlnet\": {\n",
    "            #         \"args\": [\n",
    "            #             {\n",
    "            #                 \"input_image\": pose_img,\n",
    "            #                 # \"module\": \"depth\",\"model\": \"control_depth-fp16 [400750f6]\"\n",
    "            #                 \"module\": \"none\" if pose_on else \"openpose\", \"model\": \"control_openpose-fp16 [9ca67cc5]\",\n",
    "            #             }\n",
    "            #         ]\n",
    "            #     }\n",
    "            # }\n",
    "            }\n",
    "    \n",
    "    query_url = 'http://127.0.0.1:7860/sdapi/v1/txt2img'\n",
    "    response = submit_post(query_url, data)\n",
    "\n",
    "    save_encoded_image(\n",
    "        response.json()['images'][0], img_path)\n",
    "\n",
    "\n",
    "\n",
    "for k, v in rcontent[\"npcs\"].items():\n",
    "    if \"clothes\" not in v:\n",
    "        continue\n",
    "    environment = None\n",
    "    if \"job\" in npc:\n",
    "        workplace = rcontent[\"workplaces\"][npc[\"job\"][\"workplace\"]]\n",
    "        environment = workplace[\"keywords\"]\n",
    "    # print(\"** Generating\", k)\n",
    "    prompt = render_prompt(k, v, environment)\n",
    "    render(prompt, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webuiapi\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "import base64\n",
    "import requests\n",
    "api = webuiapi.WebUIApi(host='127.0.0.1', port=7860)\n",
    "\n",
    "\n",
    "def submit_post(url: str, data: dict):\n",
    "    \"\"\"\n",
    "    Submit a POST request to the given URL with the given data.\n",
    "    \"\"\"\n",
    "    return requests.post(url, data=json.dumps(data))\n",
    "\n",
    "\n",
    "def save_encoded_image(b64_image: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Save the given image to the given output path.\n",
    "    \"\"\"\n",
    "    with open(output_path, \"wb\") as image_file:\n",
    "        image_file.write(base64.b64decode(b64_image))\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as i:\n",
    "        b64 = base64.b64encode(i.read())\n",
    "    return b64.decode(\"utf-8\")\n",
    "\n",
    "\n",
    "api.util_set_model('dreamlikeDiffusion10_10.ckpt [0aecbcfa2c]')\n",
    "# api.util_set_model('elldrethSLucidMix_v10.safetensors [67abd65708]')\n",
    "# api.util_set_model('realisticVisionV13_v13.safetensors [c35782bad8]')\n",
    "# api.util_set_model(\"RPG-v4.safetensors [e04b020012]\")\n",
    "# api.util_set_model('realisticVisionV13_v13.safetensors [c35782bad8]')\n",
    "\n",
    "negative_prompt = \"flowers, ((3d)), text, watermark, people, man, woman, fog, blur, bad art, (playing card), card, ((out of frame)) painting. cartoon. (low resolution). blurred. (computer generated). ((3d model)), ugly, ((3D render)), warped, ((watermark)), plan, blueprint, map\"\n",
    "\n",
    "def render_building_prompt(site_name, site, wk_name, wk):\n",
    "    local_desc = \", \".join(rcontent[\"details\"][\"architecture\"][\"style\"])\n",
    "    \n",
    "    desc = \"\"\n",
    "    if site[\"inherits_architecture\"]:\n",
    "        desc = \", \".join(site[\"architecture\"])\n",
    "\n",
    "    \n",
    "    prompt = f\"\"\"an award winning high angle view of a ((({site[\"state\"]} {site[\"details\"]}))), ({wk[\"prosperity\"]}), {wk['activity']}, ((({local_desc}))), medieval, {desc}, {site[\"type\"]}:0.1 concept art, ((heroic fantasy)), (illustration), fine art, digital art, (cinematic lighting), high quality, high contrast, realistic lighting, center of frame, 4k textures, (intricate), elegant, highly detailed, sharp focus, 8k, hdr, epic, sense of awe, insane details, intricate details, hyperdetailed, harsh cinematic lights, outdoor atmosphere, <lora:epiNoiseoffset_v2:1>\"\"\"\n",
    "\n",
    "    prompt_name = f\"{OUTPUT}/sites/prompts/{site_name}_{wk_name}.txt\"\n",
    "    with open(prompt_name, \"w\") as outfile:\n",
    "        outfile.write(prompt)\n",
    "        outfile.write(\"\\n\\n\")\n",
    "        outfile.write(negative_prompt)\n",
    "            \n",
    "    return [prompt]\n",
    "\n",
    "\n",
    "def render_global_prompts(content_name, archi, type):\n",
    "    prompts = []\n",
    "    site_names = []\n",
    "    parent_name = []\n",
    "    style = \", \".join(archi[\"style\"])\n",
    "    \n",
    "    view_types = [\"high angle view\", \"aerial view\", \"global view\", \"bird view\", \"street scene\"]\n",
    "    styles = [\"\", style, style, \"\", style]\n",
    "    for view_type, style in zip(view_types, styles):\n",
    "        prompt = f\"\"\"an award winning {view_type} of a ((({archi[\"global_view\"]}))), medieval, {type}:1, ({style}):0.5, concept art, ((heroic fantasy)), (illustration), fine art, digital art, (cinematic lighting), high quality, high contrast, realistic lighting, center of frame, 4k textures, (intricate), elegant, highly detailed, sharp focus, 8k, hdr, epic, sense of awe, insane details, intricate details, hyperdetailed, harsh cinematic lights, outdoor atmosphere, <lora:epiNoiseoffset_v2:1>\"\"\"\n",
    "        prompts.append(prompt)\n",
    "        site_names.append(view_type)\n",
    "        parent_name.append(content_name)\n",
    "    \n",
    "    for k, v in archi[\"sites_keywords\"].items():\n",
    "        desc = \", \".join(v)\n",
    "        prompt = f\"\"\"an award winning high angle view of a (({k})), ({desc}), {site[\"type\"]}:0.1 concept art, ((heroic fantasy)), (illustration), fine art, digital art, (cinematic lighting), high quality, high contrast, realistic lighting, center of frame, 4k textures, (intricate), elegant, highly detailed, sharp focus, 8k, hdr, epic, sense of awe, insane details, intricate details, hyperdetailed, harsh cinematic lights, outdoor atmosphere, <lora:epiNoiseoffset_v2:1>\"\"\"\n",
    "        prompts.append(prompt)\n",
    "        site_names.append(k)\n",
    "        parent_name.append(content_name)\n",
    "\n",
    "    prompt_name = f\"{OUTPUT}/sites/prompts/{content_name}.txt\"\n",
    "    with open(prompt_name, \"w\") as outfile:\n",
    "        for prompt in prompts:\n",
    "            outfile.write(prompt)\n",
    "            outfile.write(\"\\n\")\n",
    "            outfile.write(negative_prompt)\n",
    "            outfile.write(\"\\n\\n\\n\")\n",
    "            \n",
    "    return prompts, site_names, parent_name\n",
    "\n",
    "def render_building(prompts, site_names, parent_names):\n",
    "    seed = random.randint(0, 1000000)\n",
    "    width = 720\n",
    "    height = 512\n",
    "\n",
    "    for prompt, site_name, parent_name in zip(prompts, site_names, parent_names):\n",
    "        img_path = f\"{OUTPUT}/sites/{site_name}_{parent_name}.jpg\"\n",
    "        if os.path.exists(img_path):\n",
    "            # print(f\"Image {site_name} already exists, skipping.\")\n",
    "            continue\n",
    "        print(f\"Generating {site_name} view for {parent_name}.\")\n",
    "\n",
    "        data = {\"prompt\": prompt,\n",
    "                \"negative_prompt\": negative_prompt,\n",
    "                # \"init_images\": [image],\t# For img2img\n",
    "                \"sampler_name\": 'DPM++ 2S a Karras',\n",
    "                \"seed\": -1,\n",
    "                \"cfg_scale\": 6,\n",
    "                \"n_iter\": 1,\n",
    "                \"steps\": 30,\n",
    "                \"batch_size\": 1,\n",
    "                \"width\": width,\n",
    "                \"height\": height,\n",
    "                \"restore_faces\": False,\n",
    "                \"enable_hr\": True,\n",
    "                \"denoising_strength\": 0.65,\n",
    "                \"hr_scale\": 2,\n",
    "                \"hr_upscaler\": \"Latent\",\n",
    "                \"hr_resize_x\": width*2,\n",
    "                \"hr_resize_y\": height*2,\n",
    "                }\n",
    "        query_url = 'http://127.0.0.1:7860/sdapi/v1/txt2img'\n",
    "        response = submit_post(query_url, data)\n",
    "\n",
    "        save_encoded_image(\n",
    "            response.json()['images'][0], img_path)\n",
    "\n",
    "\n",
    "for k, v in rcontent[\"workplaces\"].items():\n",
    "    for site_name, site in v[\"sites\"].items():\n",
    "        prompts = render_building_prompt(site_name, site, k, v)\n",
    "        render_building(prompts, [site_name], [k])\n",
    "        \n",
    "\n",
    "prompts, site_names, parent_name = render_global_prompts(rcontent[\"name\"], rcontent[\"details\"][\"architecture\"], rcontent[\"type\"])\n",
    "render_building(prompts, site_names, parent_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "PROJECT_ID = \"Enclave\"\n",
    "OUTPUT = f\"output/{PROJECT_ID}\"\n",
    "\n",
    "RELOAD_GLOBAL = True\n",
    "\n",
    "\n",
    "if RELOAD_GLOBAL:\n",
    "    if os.path.exists(f\"{OUTPUT}/global.json\"):\n",
    "        with open(f\"{OUTPUT}/global.json\", \"r\") as infile:\n",
    "            gcontent = json.load(infile)\n",
    "else:\n",
    "    gcontent = dc(rcontent)\n",
    "\n",
    "\n",
    "groups_to_delete = []\n",
    "for wkn, wk in gcontent[\"groups\"].items():\n",
    "    key_fig = None\n",
    "    if len(wk[\"key_figures\"]) > 0:\n",
    "        key_fig = random.choice(wk[\"key_figures\"])\n",
    "    elif len(wk[\"members\"]) > 0:\n",
    "        key_fig = random.choice(wk[\"members\"])\n",
    "    else:\n",
    "        print(f\"Group {wkn} has no members or key figures, deleting.\")\n",
    "        groups_to_delete.append(wkn)\n",
    "        continue\n",
    "\n",
    "    wk[\"key_fig\"] = key_fig\n",
    "    wk[\"rtype\"] = wk[\"type\"].replace(\"_type\", \"\")\n",
    "\n",
    "    if wk[\"origin\"] == \"outsiders\":\n",
    "        prefix = wk[\"type\"].replace(\"_type\", \"\")\n",
    "        if prefix == \"solo\":\n",
    "            prefix = \"\"\n",
    "        suffix = \"\"\n",
    "        if len(wk[\"key_figures\"]) > 0:\n",
    "            suffix = wk[\"key_figures\"][0].split(\" \")[-1]\n",
    "        else:\n",
    "            suffix = wk[\"members\"][0].split(\" \")[-1]\n",
    "        wk[\"name\"] = f\"{suffix} {prefix}\"\n",
    "    else:\n",
    "        if wk[\"type\"] == \"family_type\":\n",
    "            wk[\"name\"] = wk[\"name\"] + \" family\"\n",
    "\n",
    "# for g in groups_to_delete:\n",
    "#     del gcontent[\"groups\"][g]\n",
    "\n",
    "\n",
    "to_delete = []\n",
    "for npc_name, npc in gcontent[\"npcs\"].items():\n",
    "    img_names = [f\"{npc['fullname']}\", f\"{npc['fullname'].strip()}\", \" \".join(npc[\"fullname\"].split(\" \")[::-1]), \" \".join(npc[\"fullname\"].split(\" \")[::-1]).strip()]\n",
    "    for img_name in img_names:\n",
    "        img_path = f\"portraits/{img_name}.jpg\"\n",
    "        if os.path.exists(OUTPUT + \"/\" + img_path):\n",
    "            npc[\"img_path\"] = img_path\n",
    "            break\n",
    "    if \"img_path\" not in npc:\n",
    "        print(f\"Could not find image for {npc_name}\")\n",
    "    \n",
    "    \n",
    "    if \"clothes\" not in npc:\n",
    "        to_delete.append(npc_name)\n",
    "for n in to_delete:\n",
    "    del gcontent[\"npcs\"][n]\n",
    "    \n",
    "gcontent[\"size\"] = len(list(gcontent[\"npcs\"].keys()))\n",
    "gcontent[\"key_npcs\"] = {}\n",
    "gcontent[\"lo_npcs\"] = {}\n",
    "for npc_name, npc in gcontent[\"npcs\"].items():\n",
    "    # print(npc_name)\n",
    "    key_npc = False\n",
    "    for k, v in gcontent[\"groups\"].items():\n",
    "        if npc_name in v[\"key_figures\"] or npc_name in v[\"members\"]:\n",
    "            npc[\"family\"][\"family_name\"] = v[\"name\"]\n",
    "    \n",
    "    if \"key_figure\" in npc:\n",
    "        if npc[\"key_figure\"] == \"yes\":\n",
    "            key_npc = True\n",
    "    if \"job\" in npc:\n",
    "        if \"key_figure\" in npc[\"job\"]:\n",
    "            if npc[\"job\"][\"key_figure\"] == \"yes\":\n",
    "                key_npc = True\n",
    "    if key_npc:\n",
    "        gcontent[\"key_npcs\"][npc_name] = npc\n",
    "        # gcontent[\"key_npcs\"][npc_name.strip()] = npc\n",
    "    else:\n",
    "        gcontent[\"lo_npcs\"][npc_name] = npc\n",
    "        # gcontent[\"key_npcs\"][npc_name.strip()] = npc\n",
    "\n",
    "\n",
    "for k, v in gcontent[\"workplaces\"].items():\n",
    "    for site_name, site in v[\"sites\"].items():\n",
    "        img_path = f\"sites/{site_name}_{k}.jpg\"\n",
    "        site[\"img_path\"] = img_path\n",
    "        gcontent[\"workplaces\"][k][\"sites\"][site_name] = site\n",
    "        \n",
    "\n",
    "for wkn, wk in gcontent[\"workplaces\"].items():\n",
    "    wk[\"rsite\"] = list(wk[\"sites\"].values())[0][\"img_path\"]\n",
    "\n",
    "gcontent[\"details\"][\"architecture\"][\"sites_img\"] = {}\n",
    "for k, v in gcontent[\"details\"][\"architecture\"][\"sites_details\"].items():\n",
    "    img_path = f\"sites/{k}_{gcontent['name']}.jpg\"\n",
    "    gcontent[\"details\"][\"architecture\"][\"sites_img\"][k] = img_path\n",
    "\n",
    "view_types = [\"high angle view\", \"aerial view\", \"global view\", \"bird view\", \"street scene\"]\n",
    "gcontent[\"details\"][\"global_views\"] = {}\n",
    "for view_type in view_types:\n",
    "    img_name = f\"{view_type}_{gcontent['name']}\"\n",
    "    img_path = f\"sites/{img_name}.jpg\"\n",
    "    gcontent[\"details\"][\"global_views\"][view_type] = img_path\n",
    "\n",
    "local_path = Path.cwd()\n",
    "templates_dir = os.path.join(local_path, 'templates')\n",
    "env = Environment(loader=FileSystemLoader(templates_dir))\n",
    "template = env.get_template('place.html')\n",
    "filename = os.path.join(f\"output/{PROJECT_ID}\", 'local_index.html')\n",
    "# gcontent[\"folder_path\"] = \"file:///D:/shared/onedrive_cirad/OneDrive - Cirad/Self/ttrpg_content_generator/output/B\"\n",
    "gcontent[\"folder_path\"] = \".\"\n",
    "with open(filename, 'w') as fh:\n",
    "    fh.write(template.render(\n",
    "        **gcontent\n",
    "    ))\n",
    "\n",
    "with open(f\"output/{PROJECT_ID}/global.json\", \"w\") as f:\n",
    "    json.dump(gcontent, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate PDF\n",
    "import os\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "from pathlib import Path\n",
    "import pdfkit\n",
    "import json\n",
    "\n",
    "local_path = Path.cwd()\n",
    "templates_dir = os.path.join(local_path, 'templates')\n",
    "env = Environment(loader=FileSystemLoader(templates_dir))\n",
    "template = env.get_template('pdf.html')\n",
    "\n",
    "# dir_list = os.listdir(\"output\")\n",
    "dir_list = [PROJECT_ID]\n",
    "for d in dir_list:\n",
    "    if not os.path.isdir(f\"output/{d}\"):\n",
    "        continue\n",
    "    \n",
    "    if not os.path.exists(f\"output/{d}/global.json\"):\n",
    "        print(f\"output/{d}/global.json not found\")\n",
    "        continue\n",
    "    with open(f\"output/{d}/global.json\") as ff:\n",
    "        local_content = json.load(ff)\n",
    "        filename = os.path.join(f\"output/{d}\", 'pdf.html')\n",
    "        with open(filename, 'w') as fh:\n",
    "            fh.write(template.render(\n",
    "                **local_content\n",
    "            ))\n",
    "\n",
    "        path_to_wkhtmltopdf = r\"D:\\\\shared\\\\onedrive_cirad\\\\OneDrive - Cirad\\\\Self\\\\ttrpg_content_generator\\\\pdf\\\\wkhtmltopdf.exe\"\n",
    "        path_to_file = os.path.join(f\"output/{local_content['name']}.pdf\")\n",
    "        config = pdfkit.configuration(wkhtmltopdf=path_to_wkhtmltopdf)\n",
    "        pdfkit.from_file(filename, output_path=path_to_file, configuration=config, options={\"enable-local-file-access\": \"\", \"image-dpi\": 150, \"image-quality\": 30, \"lowquality\": \"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload all photo to imgbox and regenerate the web.json and index.html\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import pyimgbox\n",
    "import logging\n",
    "import json\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "local_path = Path.cwd()\n",
    "templates_dir = os.path.join(local_path, 'templates')\n",
    "env = Environment(loader=FileSystemLoader(templates_dir))\n",
    "template = env.get_template('place.html')\n",
    "\n",
    "def save(lcontent):\n",
    "    with open(f\"output/{lcontent['project_id']}/web.json\", \"w\") as f:\n",
    "        json.dump(lcontent, f, indent=4, sort_keys=True)\n",
    "\n",
    "\n",
    "async def upload_images(lcontent, gallery, pid):\n",
    "    path = f\"./output/{pid}/\"\n",
    "    for npc_name, npc in lcontent[\"npcs\"].items():\n",
    "        print(npc_name, end=\" \")\n",
    "        if \"img_submission\" in npc:\n",
    "            print(\"skipped\")\n",
    "            continue\n",
    "        \n",
    "        img_path = path + npc[\"img_path\"]\n",
    "        submission = await gallery.upload(img_path)\n",
    "        print(submission.image_url)\n",
    "        npc[\"img_path\"] = submission.image_url\n",
    "        npc[\"img_submission\"] = str(submission)\n",
    "        \n",
    "        if npc_name in lcontent[\"lo_npcs\"]:\n",
    "            lcontent[\"lo_npcs\"][npc_name][\"img_path\"] = submission.image_url\n",
    "            lcontent[\"lo_npcs\"][npc_name][\"img_submission\"] = str(submission)\n",
    "        elif npc_name in lcontent[\"key_npcs\"]:\n",
    "            lcontent[\"key_npcs\"][npc_name][\"img_path\"] = submission.image_url\n",
    "            lcontent[\"key_npcs\"][npc_name][\"img_submission\"] = str(submission)\n",
    "        save(lcontent)\n",
    "        # break\n",
    "\n",
    "\n",
    "    for wk_name, v in lcontent[\"workplaces\"].items():\n",
    "        for site_name, site in v[\"sites\"].items():\n",
    "            print(wk_name, site_name, end=\" \")\n",
    "            if \"img_submission\" in site:\n",
    "                print(\"skipped\")\n",
    "                continue\n",
    "            \n",
    "            img_path = path + site[\"img_path\"]\n",
    "            submission = await gallery.upload(img_path)\n",
    "            print(submission.image_url)\n",
    "            site[\"img_path\"] = submission.image_url\n",
    "            site[\"img_submission\"] = str(submission)\n",
    "            save(lcontent)\n",
    "            # break\n",
    "        save(lcontent)\n",
    "        # break\n",
    "\n",
    "    for _, wk in lcontent[\"workplaces\"].items():\n",
    "        print(\"Setting rsite for\", wk[\"name\"])\n",
    "        wk[\"rsite\"] = list(wk[\"sites\"].values())[0][\"img_path\"]\n",
    "        save(lcontent)\n",
    "        # break\n",
    "\n",
    "\n",
    "    if \"sites_submission\" not in lcontent[\"details\"][\"architecture\"]:\n",
    "        lcontent[\"details\"][\"architecture\"][\"sites_submission\"] = {}\n",
    "    \n",
    "    for k, v in lcontent[\"details\"][\"architecture\"][\"sites_img\"].items():\n",
    "        print(k, end=\" \")\n",
    "        if k in lcontent[\"details\"][\"architecture\"][\"sites_submission\"]:\n",
    "            print(\"skipped\")\n",
    "            continue\n",
    "        \n",
    "        img_path = path + v\n",
    "        submission = await gallery.upload(img_path)\n",
    "        print(submission.image_url)\n",
    "        lcontent[\"details\"][\"architecture\"][\"sites_submission\"][k] = submission\n",
    "        lcontent[\"details\"][\"architecture\"][\"sites_img\"][k] = submission.image_url\n",
    "        save(lcontent)\n",
    "        # break\n",
    "    \n",
    "\n",
    "    view_types = [\"high angle view\", \"aerial view\", \"global view\", \"bird view\", \"street scene\"]\n",
    "    if \"global_submissions\" not in lcontent[\"details\"]:\n",
    "        lcontent[\"details\"][\"global_submissions\"] = {}\n",
    "\n",
    "    for view_type in view_types:\n",
    "        print(view_type, end=\" \")\n",
    "        if view_type in lcontent[\"details\"][\"global_submissions\"]:\n",
    "            print(\"skipped\")\n",
    "            continue\n",
    "        \n",
    "        img_path = path + lcontent[\"details\"][\"global_views\"][view_type]\n",
    "        print(img_path)\n",
    "        submission = await gallery.upload(img_path)\n",
    "        print(submission.image_url)\n",
    "        lcontent[\"details\"][\"global_submissions\"][view_type] = submission\n",
    "        lcontent[\"details\"][\"global_views\"][view_type] = submission.image_url\n",
    "        save(lcontent)\n",
    "        # break\n",
    "    \n",
    "    return lcontent\n",
    "\n",
    "\n",
    "async def cdn():\n",
    "    # dir_list = os.listdir(\"output\")\n",
    "    # site_list = {}\n",
    "    # for d in dir_list:\n",
    "    d = \"Enclave\"\n",
    "    # d = \"P\"\n",
    "    if os.path.isdir(f\"output/{d}\"):\n",
    "        print(f\"processing the folder {d}\")\n",
    "        \n",
    "        if not os.path.exists(f\"output/{d}/global.json\"):\n",
    "            print(f\"output/{d}/global.json not found\")\n",
    "            print(f\"skipping the folder {d}\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            with open(f\"output/{d}/global.json\") as ff:\n",
    "                local_content = json.load(ff)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"output/{d}/global.json not found\")\n",
    "        \n",
    "        if os.path.exists(f\"output/{d}/web.json\"):\n",
    "            try:\n",
    "                with open(f\"output/{d}/web.json\") as ff:\n",
    "                    local_content = json.load(ff)\n",
    "                    print(\"Using web.json\")\n",
    "            except:\n",
    "                print(f\"output/{d}/web.json not found\")\n",
    "                \n",
    "        print(local_content['name'])\n",
    "        async with pyimgbox.Gallery(title=f\"{local_content['name']}_gen_img\") as gallery:\n",
    "            try:\n",
    "                await gallery.create()\n",
    "            except ConnectionError as e:\n",
    "                print('Gallery creation failed:', str(e))\n",
    "            else:\n",
    "                print('Gallery URL:', gallery.url)\n",
    "                web_content = await upload_images(local_content, gallery, d)\n",
    "                # print(web_content[\"npcs\"])\n",
    "                filename = os.path.join(f\"output/{d}\", 'index.html')\n",
    "                with open(filename, 'w') as fh:\n",
    "                    fh.write(template.render(\n",
    "                    **web_content\n",
    "                ))\n",
    "            finally:\n",
    "                await gallery.close()\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.create_task(cdn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from jinja2 import Environment, FileSystemLoader\n",
    "# from pathlib import Path\n",
    "\n",
    "# local_path = Path.cwd()\n",
    "# templates_dir = os.path.join(local_path, 'templates')\n",
    "# env = Environment(loader=FileSystemLoader(templates_dir))\n",
    "# template = env.get_template('place.html')\n",
    "\n",
    "# d = \"G\"\n",
    "# for d in [\"B\", \"G\", \"N\", \"X\", \"Z\"]:\n",
    "#     with open(f\"output/{d}/web.json\") as f:\n",
    "#         web_content = json.load(f)\n",
    "#         filename = os.path.join(f\"output/{d}\", 'index.html')\n",
    "#         with open(filename, 'w') as fh:\n",
    "#             fh.write(template.render(\n",
    "#                 **web_content\n",
    "#             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(\"output\")\n",
    "site_list = {}\n",
    "for d in dir_list:\n",
    "    if os.path.isdir(f\"output/{d}\"):\n",
    "        # test if index.html exists\n",
    "        if os.path.exists(f\"output/{d}/index.html\"):\n",
    "            local_content = json.load(open(f\"output/{d}/web.json\"))\n",
    "            name = local_content[\"name\"]\n",
    "            site_name = list(local_content[\"details\"][\"architecture\"][\"sites_keywords\"].keys())[0]\n",
    "            desc = local_content[\"details\"][\"architecture\"][\"global_view\"]\n",
    "            img_path = {\"view\": local_content[\"details\"][\"global_views\"][\"global view\"]}\n",
    "            site_list[name] = {\n",
    "                \"id\": d,\n",
    "                \"name\": name,\n",
    "                \"desc\": desc,\n",
    "                \"site\": site_name,\n",
    "                \"size\": local_content[\"size\"],\n",
    "                \"img_path\": img_path,\n",
    "            }\n",
    "            \n",
    "jinja_obj = {\n",
    "    \"folder_path\": \".\",\n",
    "    \"sites\": site_list,\n",
    "}\n",
    "\n",
    "print(jinja_obj)\n",
    "\n",
    "local_path = Path.cwd()\n",
    "templates_dir = os.path.join(local_path, 'templates')\n",
    "env = Environment(loader=FileSystemLoader(templates_dir))\n",
    "template = env.get_template('index.html')\n",
    "filename = os.path.join(f\"output\", 'index.html')\n",
    "with open(filename, 'w') as fh:\n",
    "    fh.write(template.render(\n",
    "        **jinja_obj\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Correct image silhouettes\n",
    "\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "\n",
    "# folder = \"openpose_db\"\n",
    "# dir_list = os.listdir(folder)\n",
    "# cat = [[] for i in range(5)]\n",
    "# for n in dir_list:\n",
    "#     # get unique value for each word separated by _\n",
    "#     if \"boy\" in n:\n",
    "#         os.rename(pjoin(folder, n), pjoin(folder, n.replace(\"boy\", \"child\")))\n",
    "#         print(\"renaming\", n)\n",
    "    \n",
    "#     words = n.split(\"_\")[:-1]\n",
    "#     for i, w in enumerate(words):\n",
    "#         if w not in cat[i]:\n",
    "#             cat[i].append(w)\n",
    "# print(cat)\n",
    "\n",
    "\n",
    "# pattern = \"['dwarf', 'elf', 'gnome', 'human', 'half_elf', 'halfling']_['female', 'male']_['child', 'elderly', 'infant', 'middle aged', 'preteen', 'teenager', 'thirties', 'very old', 'young adult']_['fat', 'muscular', 'normal', 'skinny']_['normal', 'small', 'tall']\"\n",
    "\n",
    "import json\n",
    "\n",
    "import query\n",
    "import prompt as P\n",
    "from parsers import json_parser, table_parser\n",
    "from logger import Logger\n",
    "from llm import ChatGPT\n",
    "import defines as D\n",
    "import data_formatter as DF\n",
    "Logger = Logger()\n",
    "\n",
    "FORCE_HUMAN_READABLE_CACHE = True\n",
    "\n",
    "with open(\"openai.key\", \"r\") as f:\n",
    "    api_key = f.readline().strip()\n",
    "    api_id = f.readline().strip()\n",
    "    \n",
    "LLM = ChatGPT(Logger, \"Your an assistant to generate data for a medieval fantasy tabletop rpg game\", model_name=\"gpt-3.5-turbo\", api_key=api_key, api_id=api_id)\n",
    "Q = query.LLM_Query(LLM, Logger)\n",
    "\n",
    "\n",
    "instructions = \"\"\"This is a list of rpg characters (heroic fantasy) and I want you to translate and reorganize terms.\n",
    "\n",
    "I want you to reformat and modify the list to have a new one with the following columns:\n",
    "\n",
    "fullname|age|size|body_type\n",
    "\n",
    "For each columns the allowed values are as follows:\n",
    "- for age: [infant, child, preteen, teenager, young adult, thirties, middle aged, very old, elderly]\n",
    "- for size: [normal, small, tall]\n",
    "- for body-type: [fat, muscular, normal, skinny]\n",
    "\n",
    "Keep only the authorized values !\n",
    "\n",
    "your list:\n",
    "\n",
    "\"\"\"\n",
    "dir_list = os.listdir(\"output\")\n",
    "site_list = {}\n",
    "characs = [\"fullname\", \"age\", \"age_look\", \"race\", \"gender\", \"beauty\", \"height\", \"weight\"]\n",
    "with open(\"output/characters_correction.txt\", \"w\") as f:\n",
    "    count = 1\n",
    "    query_string = \"|\".join(characs) + \"\\n\"\n",
    "    for d in dir_list:\n",
    "        if os.path.isdir(f\"output/{d}\"):\n",
    "            local_content = json.load(open(f\"output/{d}/global.json\"))\n",
    "            for name, npc in local_content[\"npcs\"].items():\n",
    "                # f.write(\"|\".join([npc[c] for c in characs]) + \"\\n\")\n",
    "                query_string += \"|\".join([npc[c] for c in characs]) + \"\\n\"\n",
    "\n",
    "                if count % 100 == 0:\n",
    "                    query_string += instructions\n",
    "                    content, response = LLM.query(query_string, options={\"id\": \"all\", \"type\": \"correction\"})\n",
    "                    f.write(content + \"\\n\")\n",
    "                    query_string = \"|\".join(characs) + \"\\n\"\n",
    "                \n",
    "                count += 1\n",
    "f.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "def queue_thread():\n",
    "    while True:\n",
    "        for function in queue:\n",
    "            function()\n",
    "            time.sleep(40)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "queue_thread = Thread(name=\"queue\", target=lambda: queue_thread())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pynirsENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
